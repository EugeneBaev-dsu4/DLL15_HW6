{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb03f0f",
   "metadata": {},
   "source": [
    "### Домашнее задание 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974be5d",
   "metadata": {},
   "source": [
    "Сгенерировать последовательности, которые бы состояли из цифр (от 0 до 9)\n",
    "и задавались следующим образом:\n",
    "\n",
    "x - последовательность цифр\n",
    "\n",
    "y1 = x1, y(i) = x(i) + x(1). \n",
    "\n",
    "Если y(i) >= 10, то y(i) = y(i) - 10\n",
    "\n",
    "Задача:\n",
    "научить модель предсказывать y(i) по x(i)\n",
    "пробовать RNN, LSTM, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "835cdba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa13466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [int(char) for char in '0123456789']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f4b7fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 128\n",
    "message_length = 5\n",
    "\n",
    "\n",
    "def dataset(num_examples):\n",
    "    dataset = []\n",
    "    \n",
    "    for ex in range(num_examples):     \n",
    "        x = [random.choice(vocab) for x in range(message_length)]\n",
    "        \n",
    "        y = [x[0]]\n",
    "        for xi in x[1:]:\n",
    "            if (xi + x[0]) >= 10:\n",
    "                y.append((xi + x[0])%10)\n",
    "            else:\n",
    "                y.append(xi + x[0])        \n",
    "        \n",
    "        dataset.append([torch.tensor(x), torch.tensor(y)])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf08dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 5, 5, 9, 2]), tensor([1, 6, 6, 0, 3])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset(num_examples)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7258a4bb",
   "metadata": {},
   "source": [
    "#### Задача: научить модель предсказывать y(i) по x(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f98764cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 10\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82998a69",
   "metadata": {},
   "source": [
    "##### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32b21158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        ## Здесь создать слои\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, sentences, state=None):\n",
    "        embed = self.embed(sentences)\n",
    "        o, s = self.rnn(embed)\n",
    "        out = self.linear(o)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98570908",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "316cc64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e233da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.435, Train loss: 2.360\n",
      "Epoch 1. Time: 0.354, Train loss: 2.408\n",
      "Epoch 2. Time: 0.352, Train loss: 2.441\n",
      "Epoch 3. Time: 0.343, Train loss: 2.422\n",
      "Epoch 4. Time: 0.384, Train loss: 2.344\n",
      "Epoch 5. Time: 0.438, Train loss: 2.473\n",
      "Epoch 6. Time: 0.337, Train loss: 2.393\n",
      "Epoch 7. Time: 0.319, Train loss: 2.448\n",
      "Epoch 8. Time: 0.350, Train loss: 2.432\n",
      "Epoch 9. Time: 0.799, Train loss: 2.381\n",
      "Epoch 10. Time: 0.577, Train loss: 2.435\n",
      "Epoch 11. Time: 0.461, Train loss: 2.412\n",
      "Epoch 12. Time: 0.582, Train loss: 2.396\n",
      "Epoch 13. Time: 0.306, Train loss: 2.437\n",
      "Epoch 14. Time: 0.310, Train loss: 2.475\n",
      "Epoch 15. Time: 0.287, Train loss: 2.446\n",
      "Epoch 16. Time: 0.289, Train loss: 2.409\n",
      "Epoch 17. Time: 0.291, Train loss: 2.456\n",
      "Epoch 18. Time: 0.329, Train loss: 2.433\n",
      "Epoch 19. Time: 0.412, Train loss: 2.416\n",
      "Epoch 20. Time: 0.350, Train loss: 2.393\n",
      "Epoch 21. Time: 0.353, Train loss: 2.425\n",
      "Epoch 22. Time: 0.348, Train loss: 2.462\n",
      "Epoch 23. Time: 0.350, Train loss: 2.419\n",
      "Epoch 24. Time: 0.349, Train loss: 2.435\n",
      "Epoch 25. Time: 0.330, Train loss: 2.425\n",
      "Epoch 26. Time: 0.353, Train loss: 2.452\n",
      "Epoch 27. Time: 0.453, Train loss: 2.421\n",
      "Epoch 28. Time: 0.342, Train loss: 2.365\n",
      "Epoch 29. Time: 0.355, Train loss: 2.387\n",
      "Epoch 30. Time: 0.332, Train loss: 2.403\n",
      "Epoch 31. Time: 0.377, Train loss: 2.385\n",
      "Epoch 32. Time: 0.441, Train loss: 2.454\n",
      "Epoch 33. Time: 0.367, Train loss: 2.378\n",
      "Epoch 34. Time: 0.368, Train loss: 2.413\n",
      "Epoch 35. Time: 0.418, Train loss: 2.331\n",
      "Epoch 36. Time: 0.760, Train loss: 2.398\n",
      "Epoch 37. Time: 0.544, Train loss: 2.372\n",
      "Epoch 38. Time: 0.580, Train loss: 2.457\n",
      "Epoch 39. Time: 0.570, Train loss: 2.403\n",
      "Epoch 40. Time: 0.618, Train loss: 2.398\n",
      "Epoch 41. Time: 0.507, Train loss: 2.422\n",
      "Epoch 42. Time: 0.510, Train loss: 2.349\n",
      "Epoch 43. Time: 0.570, Train loss: 2.425\n",
      "Epoch 44. Time: 0.462, Train loss: 2.435\n",
      "Epoch 45. Time: 0.329, Train loss: 2.411\n",
      "Epoch 46. Time: 0.345, Train loss: 2.427\n",
      "Epoch 47. Time: 0.419, Train loss: 2.379\n",
      "Epoch 48. Time: 0.330, Train loss: 2.428\n",
      "Epoch 49. Time: 0.339, Train loss: 2.480\n",
      "Epoch 50. Time: 0.393, Train loss: 2.431\n",
      "Epoch 51. Time: 0.384, Train loss: 2.428\n",
      "Epoch 52. Time: 0.285, Train loss: 2.400\n",
      "Epoch 53. Time: 0.273, Train loss: 2.439\n",
      "Epoch 54. Time: 0.279, Train loss: 2.440\n",
      "Epoch 55. Time: 0.297, Train loss: 2.434\n",
      "Epoch 56. Time: 0.274, Train loss: 2.410\n",
      "Epoch 57. Time: 0.368, Train loss: 2.382\n",
      "Epoch 58. Time: 0.336, Train loss: 2.478\n",
      "Epoch 59. Time: 0.399, Train loss: 2.467\n",
      "Epoch 60. Time: 0.322, Train loss: 2.448\n",
      "Epoch 61. Time: 0.354, Train loss: 2.418\n",
      "Epoch 62. Time: 0.356, Train loss: 2.432\n",
      "Epoch 63. Time: 0.337, Train loss: 2.444\n",
      "Epoch 64. Time: 0.391, Train loss: 2.415\n",
      "Epoch 65. Time: 0.421, Train loss: 2.380\n",
      "Epoch 66. Time: 0.417, Train loss: 2.438\n",
      "Epoch 67. Time: 0.345, Train loss: 2.381\n",
      "Epoch 68. Time: 0.390, Train loss: 2.358\n",
      "Epoch 69. Time: 0.330, Train loss: 2.435\n",
      "Epoch 70. Time: 0.390, Train loss: 2.378\n",
      "Epoch 71. Time: 0.367, Train loss: 2.420\n",
      "Epoch 72. Time: 0.355, Train loss: 2.414\n",
      "Epoch 73. Time: 0.363, Train loss: 2.423\n",
      "Epoch 74. Time: 0.352, Train loss: 2.410\n",
      "Epoch 75. Time: 0.345, Train loss: 2.355\n",
      "Epoch 76. Time: 0.381, Train loss: 2.441\n",
      "Epoch 77. Time: 0.351, Train loss: 2.415\n",
      "Epoch 78. Time: 0.335, Train loss: 2.396\n",
      "Epoch 79. Time: 0.355, Train loss: 2.440\n",
      "Epoch 80. Time: 0.350, Train loss: 2.424\n",
      "Epoch 81. Time: 0.353, Train loss: 2.430\n",
      "Epoch 82. Time: 0.331, Train loss: 2.454\n",
      "Epoch 83. Time: 0.381, Train loss: 2.419\n",
      "Epoch 84. Time: 0.368, Train loss: 2.407\n",
      "Epoch 85. Time: 0.371, Train loss: 2.412\n",
      "Epoch 86. Time: 0.329, Train loss: 2.443\n",
      "Epoch 87. Time: 0.333, Train loss: 2.390\n",
      "Epoch 88. Time: 0.371, Train loss: 2.398\n",
      "Epoch 89. Time: 0.320, Train loss: 2.438\n",
      "Epoch 90. Time: 0.342, Train loss: 2.408\n",
      "Epoch 91. Time: 0.342, Train loss: 2.448\n",
      "Epoch 92. Time: 0.361, Train loss: 2.377\n",
      "Epoch 93. Time: 0.360, Train loss: 2.407\n",
      "Epoch 94. Time: 0.331, Train loss: 2.428\n",
      "Epoch 95. Time: 0.349, Train loss: 2.515\n",
      "Epoch 96. Time: 0.443, Train loss: 2.360\n",
      "Epoch 97. Time: 0.351, Train loss: 2.394\n",
      "Epoch 98. Time: 0.287, Train loss: 2.434\n",
      "Epoch 99. Time: 0.316, Train loss: 2.383\n"
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    for x, y in dataset(num_examples):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        answers = model.forward(x.unsqueeze(1))\n",
    "        answers = answers.view(-1, vocab_size)\n",
    "        loss = criterion(answers, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c4fc7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 15.78%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        matches, total = 0, 0\n",
    "        for x, y in dataset(num_examples):\n",
    "            answers = model.forward(x.unsqueeze(1))\n",
    "            predictions = torch.nn.functional.softmax(answers, dim=2)\n",
    "            _, batch_out = predictions.max(dim=2)\n",
    "            batch_out = batch_out.squeeze(1)\n",
    "            matches += torch.eq(batch_out, y).sum().item()\n",
    "            total += torch.numel(batch_out)\n",
    "        accuracy = matches / total\n",
    "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63151e",
   "metadata": {},
   "source": [
    "##### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a04c3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        ## Здесь создать слои\n",
    "        self.embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = torch.nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, sentences, state=None):\n",
    "        embed = self.embed(sentences)\n",
    "        o, s = self.rnn(embed)\n",
    "        out = self.linear(o)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3366af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0891440",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2fdf247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Time: 0.398, Train loss: 2.297\n",
      "Epoch 1. Time: 0.408, Train loss: 2.280\n",
      "Epoch 2. Time: 0.454, Train loss: 2.261\n",
      "Epoch 3. Time: 0.499, Train loss: 2.289\n",
      "Epoch 4. Time: 0.451, Train loss: 2.277\n",
      "Epoch 5. Time: 0.389, Train loss: 2.205\n",
      "Epoch 6. Time: 0.682, Train loss: 2.286\n",
      "Epoch 7. Time: 0.793, Train loss: 2.244\n",
      "Epoch 8. Time: 0.413, Train loss: 2.313\n",
      "Epoch 9. Time: 0.341, Train loss: 2.296\n",
      "Epoch 10. Time: 0.350, Train loss: 2.211\n",
      "Epoch 11. Time: 0.370, Train loss: 2.256\n",
      "Epoch 12. Time: 0.652, Train loss: 2.261\n",
      "Epoch 13. Time: 0.523, Train loss: 2.285\n",
      "Epoch 14. Time: 0.361, Train loss: 2.247\n",
      "Epoch 15. Time: 0.400, Train loss: 2.211\n",
      "Epoch 16. Time: 0.413, Train loss: 2.305\n",
      "Epoch 17. Time: 0.491, Train loss: 2.299\n",
      "Epoch 18. Time: 0.417, Train loss: 2.331\n",
      "Epoch 19. Time: 0.406, Train loss: 2.257\n",
      "Epoch 20. Time: 0.510, Train loss: 2.319\n",
      "Epoch 21. Time: 0.394, Train loss: 2.283\n",
      "Epoch 22. Time: 0.409, Train loss: 2.247\n",
      "Epoch 23. Time: 0.380, Train loss: 2.291\n",
      "Epoch 24. Time: 0.421, Train loss: 2.309\n",
      "Epoch 25. Time: 0.403, Train loss: 2.280\n",
      "Epoch 26. Time: 0.400, Train loss: 2.299\n",
      "Epoch 27. Time: 0.413, Train loss: 2.293\n",
      "Epoch 28. Time: 0.376, Train loss: 2.289\n",
      "Epoch 29. Time: 0.403, Train loss: 2.259\n",
      "Epoch 30. Time: 0.334, Train loss: 2.271\n",
      "Epoch 31. Time: 0.431, Train loss: 2.264\n",
      "Epoch 32. Time: 0.372, Train loss: 2.273\n",
      "Epoch 33. Time: 0.372, Train loss: 2.273\n",
      "Epoch 34. Time: 0.384, Train loss: 2.333\n",
      "Epoch 35. Time: 0.383, Train loss: 2.277\n",
      "Epoch 36. Time: 0.393, Train loss: 2.300\n",
      "Epoch 37. Time: 0.401, Train loss: 2.215\n",
      "Epoch 38. Time: 0.408, Train loss: 2.258\n",
      "Epoch 39. Time: 0.408, Train loss: 2.243\n",
      "Epoch 40. Time: 0.450, Train loss: 2.251\n",
      "Epoch 41. Time: 0.417, Train loss: 2.244\n",
      "Epoch 42. Time: 0.389, Train loss: 2.255\n",
      "Epoch 43. Time: 0.447, Train loss: 2.262\n",
      "Epoch 44. Time: 0.404, Train loss: 2.256\n",
      "Epoch 45. Time: 0.358, Train loss: 2.293\n",
      "Epoch 46. Time: 0.407, Train loss: 2.272\n",
      "Epoch 47. Time: 0.426, Train loss: 2.308\n",
      "Epoch 48. Time: 0.412, Train loss: 2.237\n",
      "Epoch 49. Time: 0.395, Train loss: 2.298\n",
      "Epoch 50. Time: 0.431, Train loss: 2.309\n",
      "Epoch 51. Time: 0.381, Train loss: 2.273\n",
      "Epoch 52. Time: 0.381, Train loss: 2.297\n",
      "Epoch 53. Time: 0.416, Train loss: 2.235\n",
      "Epoch 54. Time: 0.411, Train loss: 2.303\n",
      "Epoch 55. Time: 0.473, Train loss: 2.287\n",
      "Epoch 56. Time: 0.389, Train loss: 2.281\n",
      "Epoch 57. Time: 0.412, Train loss: 2.312\n",
      "Epoch 58. Time: 0.410, Train loss: 2.327\n",
      "Epoch 59. Time: 0.371, Train loss: 2.300\n",
      "Epoch 60. Time: 0.433, Train loss: 2.315\n",
      "Epoch 61. Time: 0.419, Train loss: 2.326\n",
      "Epoch 62. Time: 0.397, Train loss: 2.279\n",
      "Epoch 63. Time: 0.383, Train loss: 2.287\n",
      "Epoch 64. Time: 0.506, Train loss: 2.303\n",
      "Epoch 65. Time: 0.396, Train loss: 2.279\n",
      "Epoch 66. Time: 0.387, Train loss: 2.293\n",
      "Epoch 67. Time: 0.417, Train loss: 2.348\n",
      "Epoch 68. Time: 0.414, Train loss: 2.330\n",
      "Epoch 69. Time: 0.315, Train loss: 2.386\n",
      "Epoch 70. Time: 0.320, Train loss: 2.279\n",
      "Epoch 71. Time: 0.345, Train loss: 2.323\n",
      "Epoch 72. Time: 0.286, Train loss: 2.297\n",
      "Epoch 73. Time: 0.319, Train loss: 2.315\n",
      "Epoch 74. Time: 0.398, Train loss: 2.336\n",
      "Epoch 75. Time: 0.397, Train loss: 2.368\n",
      "Epoch 76. Time: 0.417, Train loss: 2.329\n",
      "Epoch 77. Time: 0.374, Train loss: 2.308\n",
      "Epoch 78. Time: 0.402, Train loss: 2.310\n",
      "Epoch 79. Time: 0.409, Train loss: 2.318\n",
      "Epoch 80. Time: 0.441, Train loss: 2.345\n",
      "Epoch 81. Time: 0.490, Train loss: 2.385\n",
      "Epoch 82. Time: 0.399, Train loss: 2.306\n",
      "Epoch 83. Time: 0.395, Train loss: 2.309\n",
      "Epoch 84. Time: 0.368, Train loss: 2.334\n",
      "Epoch 85. Time: 0.407, Train loss: 2.302\n",
      "Epoch 86. Time: 0.405, Train loss: 2.313\n",
      "Epoch 87. Time: 0.741, Train loss: 2.315\n",
      "Epoch 88. Time: 0.421, Train loss: 2.358\n",
      "Epoch 89. Time: 0.401, Train loss: 2.313\n",
      "Epoch 90. Time: 0.401, Train loss: 2.370\n",
      "Epoch 91. Time: 0.411, Train loss: 2.348\n",
      "Epoch 92. Time: 0.381, Train loss: 2.341\n",
      "Epoch 93. Time: 0.367, Train loss: 2.318\n",
      "Epoch 94. Time: 0.418, Train loss: 2.320\n",
      "Epoch 95. Time: 0.416, Train loss: 2.345\n",
      "Epoch 96. Time: 0.503, Train loss: 2.346\n",
      "Epoch 97. Time: 0.392, Train loss: 2.319\n",
      "Epoch 98. Time: 0.392, Train loss: 2.314\n",
      "Epoch 99. Time: 0.430, Train loss: 2.309\n"
     ]
    }
   ],
   "source": [
    "for ep in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss = 0.\n",
    "    train_passed = 0\n",
    "\n",
    "    for x, y in dataset(num_examples):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        answers = model.forward(x.unsqueeze(1))\n",
    "        answers = answers.view(-1, vocab_size)\n",
    "        loss = criterion(answers, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_passed += 1\n",
    "\n",
    "    print(\"Epoch {}. Time: {:.3f}, Train loss: {:.3f}\".format(ep, time.time() - start, train_loss / train_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fecd5333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.22%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        matches, total = 0, 0\n",
    "        for x, y in dataset(num_examples):\n",
    "            answers = model.forward(x.unsqueeze(1))\n",
    "            predictions = torch.nn.functional.softmax(answers, dim=2)\n",
    "            _, batch_out = predictions.max(dim=2)\n",
    "            batch_out = batch_out.squeeze(1)\n",
    "            matches += torch.eq(batch_out, y).sum().item()\n",
    "            total += torch.numel(batch_out)\n",
    "        accuracy = matches / total\n",
    "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b667b1a2",
   "metadata": {},
   "source": [
    "##### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37c5d69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "lstm = torch.nn.LSTM(embedding_dim, hidden_dim)\n",
    "linear = torch.nn.Linear(hidden_dim, vocab_size)\n",
    "softmax = torch.nn.functional.softmax\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(list(embed.parameters()) +\n",
    "                             list(lstm.parameters()) +\n",
    "                             list(linear.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "406446f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_hidden():\n",
    "    return (torch.zeros(1, 1, hidden_dim),\n",
    "            torch.zeros(1, 1, hidden_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8f06c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: 2.3471\n",
      "Epoch: 1\n",
      "Loss: 2.1080\n",
      "Epoch: 2\n",
      "Loss: 2.0135\n",
      "Epoch: 3\n",
      "Loss: 2.2321\n",
      "Epoch: 4\n",
      "Loss: 2.2405\n",
      "Epoch: 5\n",
      "Loss: 1.3513\n",
      "Epoch: 6\n",
      "Loss: 2.3529\n",
      "Epoch: 7\n",
      "Loss: 2.3158\n",
      "Epoch: 8\n",
      "Loss: 2.0253\n",
      "Epoch: 9\n",
      "Loss: 1.8456\n",
      "Epoch: 10\n",
      "Loss: 1.7184\n",
      "Epoch: 11\n",
      "Loss: 2.4171\n",
      "Epoch: 12\n",
      "Loss: 1.8004\n",
      "Epoch: 13\n",
      "Loss: 2.1888\n",
      "Epoch: 14\n",
      "Loss: 1.3082\n",
      "Epoch: 15\n",
      "Loss: 1.7454\n",
      "Epoch: 16\n",
      "Loss: 1.5385\n",
      "Epoch: 17\n",
      "Loss: 1.3441\n",
      "Epoch: 18\n",
      "Loss: 1.5405\n",
      "Epoch: 19\n",
      "Loss: 2.5192\n",
      "Epoch: 20\n",
      "Loss: 2.1221\n",
      "Epoch: 21\n",
      "Loss: 2.4481\n",
      "Epoch: 22\n",
      "Loss: 2.3196\n",
      "Epoch: 23\n",
      "Loss: 2.0616\n",
      "Epoch: 24\n",
      "Loss: 1.9251\n",
      "Epoch: 25\n",
      "Loss: 1.6729\n",
      "Epoch: 26\n",
      "Loss: 1.1197\n",
      "Epoch: 27\n",
      "Loss: 1.8560\n",
      "Epoch: 28\n",
      "Loss: 1.8745\n",
      "Epoch: 29\n",
      "Loss: 0.9057\n",
      "Epoch: 30\n",
      "Loss: 2.5699\n",
      "Epoch: 31\n",
      "Loss: 1.7453\n",
      "Epoch: 32\n",
      "Loss: 1.3882\n",
      "Epoch: 33\n",
      "Loss: 2.2684\n",
      "Epoch: 34\n",
      "Loss: 1.8724\n",
      "Epoch: 35\n",
      "Loss: 1.1253\n",
      "Epoch: 36\n",
      "Loss: 1.5858\n",
      "Epoch: 37\n",
      "Loss: 1.9263\n",
      "Epoch: 38\n",
      "Loss: 1.2725\n",
      "Epoch: 39\n",
      "Loss: 2.6276\n",
      "Epoch: 40\n",
      "Loss: 3.0020\n",
      "Epoch: 41\n",
      "Loss: 1.8808\n",
      "Epoch: 42\n",
      "Loss: 1.9027\n",
      "Epoch: 43\n",
      "Loss: 2.2673\n",
      "Epoch: 44\n",
      "Loss: 2.8070\n",
      "Epoch: 45\n",
      "Loss: 2.6083\n",
      "Epoch: 46\n",
      "Loss: 1.6384\n",
      "Epoch: 47\n",
      "Loss: 1.7216\n",
      "Epoch: 48\n",
      "Loss: 2.8617\n",
      "Epoch: 49\n",
      "Loss: 2.2267\n",
      "Epoch: 50\n",
      "Loss: 1.7820\n",
      "Epoch: 51\n",
      "Loss: 2.0721\n",
      "Epoch: 52\n",
      "Loss: 2.0223\n",
      "Epoch: 53\n",
      "Loss: 2.6443\n",
      "Epoch: 54\n",
      "Loss: 1.8250\n",
      "Epoch: 55\n",
      "Loss: 1.7362\n",
      "Epoch: 56\n",
      "Loss: 2.8258\n",
      "Epoch: 57\n",
      "Loss: 1.9412\n",
      "Epoch: 58\n",
      "Loss: 1.5906\n",
      "Epoch: 59\n",
      "Loss: 1.0845\n",
      "Epoch: 60\n",
      "Loss: 2.0345\n",
      "Epoch: 61\n",
      "Loss: 2.3612\n",
      "Epoch: 62\n",
      "Loss: 2.6874\n",
      "Epoch: 63\n",
      "Loss: 2.4013\n",
      "Epoch: 64\n",
      "Loss: 1.7302\n",
      "Epoch: 65\n",
      "Loss: 1.8786\n",
      "Epoch: 66\n",
      "Loss: 2.0912\n",
      "Epoch: 67\n",
      "Loss: 1.7138\n",
      "Epoch: 68\n",
      "Loss: 2.9804\n",
      "Epoch: 69\n",
      "Loss: 1.9784\n",
      "Epoch: 70\n",
      "Loss: 2.0956\n",
      "Epoch: 71\n",
      "Loss: 1.1238\n",
      "Epoch: 72\n",
      "Loss: 1.7478\n",
      "Epoch: 73\n",
      "Loss: 2.9081\n",
      "Epoch: 74\n",
      "Loss: 2.5916\n",
      "Epoch: 75\n",
      "Loss: 1.5127\n",
      "Epoch: 76\n",
      "Loss: 2.2050\n",
      "Epoch: 77\n",
      "Loss: 2.2289\n",
      "Epoch: 78\n",
      "Loss: 2.0741\n",
      "Epoch: 79\n",
      "Loss: 2.1205\n",
      "Epoch: 80\n",
      "Loss: 2.9563\n",
      "Epoch: 81\n",
      "Loss: 1.2875\n",
      "Epoch: 82\n",
      "Loss: 1.3593\n",
      "Epoch: 83\n",
      "Loss: 2.5888\n",
      "Epoch: 84\n",
      "Loss: 2.4301\n",
      "Epoch: 85\n",
      "Loss: 2.2215\n",
      "Epoch: 86\n",
      "Loss: 2.2137\n",
      "Epoch: 87\n",
      "Loss: 2.2635\n",
      "Epoch: 88\n",
      "Loss: 1.6374\n",
      "Epoch: 89\n",
      "Loss: 1.8340\n",
      "Epoch: 90\n",
      "Loss: 2.2327\n",
      "Epoch: 91\n",
      "Loss: 1.8152\n",
      "Epoch: 92\n",
      "Loss: 2.6933\n",
      "Epoch: 93\n",
      "Loss: 2.2161\n",
      "Epoch: 94\n",
      "Loss: 2.2459\n",
      "Epoch: 95\n",
      "Loss: 1.9507\n",
      "Epoch: 96\n",
      "Loss: 1.5033\n",
      "Epoch: 97\n",
      "Loss: 2.4088\n",
      "Epoch: 98\n",
      "Loss: 2.2833\n",
      "Epoch: 99\n",
      "Loss: 2.0773\n"
     ]
    }
   ],
   "source": [
    "accuracies, max_accuracy = [], 0\n",
    "for ep in range(num_epochs):\n",
    "    print('Epoch: {}'.format(ep))\n",
    "    for x, y in dataset(num_examples):\n",
    "        lstm_in = embed(x)\n",
    "        lstm_in = lstm_in.unsqueeze(1)\n",
    "        lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\n",
    "        scores = linear(lstm_out)\n",
    "        scores = scores.transpose(1, 2)\n",
    "        y = y.unsqueeze(1)\n",
    "        loss = loss_fn(scores, y) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Loss: {:6.4f}'.format(loss.item()))\n",
    "Epoch: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efae0427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 29.69%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        matches, total = 0, 0\n",
    "        for x, y in dataset(num_examples):\n",
    "            lstm_in = embed(x)\n",
    "            lstm_in = lstm_in.unsqueeze(1)\n",
    "            lstm_out, lstm_hidden = lstm(lstm_in, zero_hidden())\n",
    "            scores = linear(lstm_out)\n",
    "            predictions = softmax(scores, dim=2)\n",
    "            _, batch_out = predictions.max(dim=2)\n",
    "            batch_out = batch_out.squeeze(1)\n",
    "            matches += torch.eq(batch_out, y).sum().item()\n",
    "            total += torch.numel(batch_out)\n",
    "        accuracy = matches / total\n",
    "        print('Accuracy: {:4.2f}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051836cd",
   "metadata": {},
   "source": [
    "len 100 100e RNN 15.78% GRU 14.22% LSTM 29.69%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42bf62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
